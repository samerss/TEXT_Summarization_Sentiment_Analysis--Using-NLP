# -*- coding: utf-8 -*-
"""Text_Summarization_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NOApfepwg7VM1fTAVy3eBjg0hNmP7602

now you don't need to read the whole news, article, post

and don't think about negativity

our program can tell if this text positive or negative
also summarize your text to percentage you choose

Welcome to our program!

Our program is designed to help you analyze and summarize text data quickly and easily. Whether you are reading a news article, blog post, or any other type of text data, our program can help you determine the sentiment of the text and summarize its content based on your chosen percentage.

Sentiment Analysis:
Our program uses natural language processing (NLP) techniques and machine learning models to analyze the sentiment of the input text. You don't need to worry about negativity, our program can determine whether the text is positive, negative, or neutral. This can be especially useful for businesses or individuals who want to quickly analyze customer feedback or social media posts.

Summarization:
Our program can also summarize the input text to a percentage that you choose. This means that you don't need to read the entire text to get the gist of its content. Simply input the text and choose the percentage of the summary you want, and our program will do the rest.

Overall, our program is a powerful tool that can save you time and help you make informed decisions based on the sentiment and content of your text data. Give it a try today and see how it can help you!
"""

# !pip instlla -U spacy
# !python -m spacy download en_core_web_sm

import spacy #analyzing text data, including tokenization, part-of-speech tagging, named entity recognition, dependency parsing
from spacy.lang.en.stop_words import STOP_WORDS #to get stop words
from string import punctuation #to get punc

with open('/content/The_city_of_violent.txt', 'r') as f:
    file_contents = f.read()

# Assign the contents to a variable
text = file_contents

#get stop words
stopwords = list(STOP_WORDS)
print(stopwords)

#english core web summarization get model
nlp = spacy.load('en_core_web_sm')

doc = nlp(text)

#get the tokens
tokens = [token.text for token in doc]
print(tokens)

#returns the punctuation string variable, new line
punctuation = punctuation + '\n'
punctuation

word_frequencies = {} # Initialize an empty dictionary to store word frequencies
# Iterate over each word in the input text
for word in doc:
  if word.text.lower() not in stopwords: # Skip stopwords
    if word.text.lower() not in punctuation: # Skip punctuation characters
      if word.text not in word_frequencies.keys():
        word_frequencies[word.text] = 1  # If the word is not already in the dictionary, add it with a frequency of 1
      else:
        word_frequencies[word.text] += 1   #If the word is already in the dictionary, increment its frequency by 1

#dis word freq
print(word_frequencies)

# Find the maximum frequency value in the word frequencies dictionary
max_frequency = max(word_frequencies.values())

max_frequency

# Normalize word frequencies by dividing each frequency by the maximum frequency
for word in word_frequencies.keys():
  word_frequencies[word] = word_frequencies[word]/max_frequency

#DIS
print(word_frequencies)

# Split the input text into a list of sentences using Spacy
sentence_tokens = [sent for sent in doc.sents]
print(sentence_tokens)

# Initialize an empty dictionary to store sentence scores
sentence_scores = {}
# Calculate the score of each sentence based on the frequency of the words it contains
for sent in sentence_tokens:
  for word in sent:
    if word.text.lower() in word_frequencies.keys():
      if sent not in sentence_scores.keys():
        #if statement checks if the lowercase version of the current word is in the word_frequencies dictionary.
        #If the word is not in the dictionary, it is skipped and not counted in the sentence score calculation.
        sentence_scores[sent] = word_frequencies[word.text.lower()] 
      else:
        sentence_scores[sent] += word_frequencies[word.text.lower()]

sentence_scores

#useful for sorting and selecting the top elements from a collection. 
from heapq import nlargest

#ADJUST LEN
select_length = int(len(sentence_tokens)*0.3)
select_length

summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)

summary

#This creates a new list named final_summary by iterating over each object in the summary list
#and extracting its text using the text attribute. When we print the final_summary list to the console, we get the following output:
final_summary = [word.text for word in summary]

#This line joins the text of each sentence in the "final_summary" list together into a single string, separated by spaces. This creates the final summary of the text.
summary = ' '.join(final_summary)

print(text)

print(summary)

from textblob import TextBlob
import nltk
nltk.download('punkt')

from textblob import TextBlob
import nltk
nltk.download('punkt')
blob = TextBlob(text)

for sentence in blob.sentences:
    print(sentence, sentence.sentiment.polarity)
def get_sentiment(text):
    """
    Returns the sentiment of the input text as a string ("positive", "negative", or "neutral").
    """
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    if polarity > 0:
        return "positive"
    elif polarity < 0:
        return "negative"
    else:
        return "neutral"  #error handling 

sentiment = get_sentiment(text)
print(sentiment)
blob = TextBlob(text)